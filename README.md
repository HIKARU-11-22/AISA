
# A.I.S.A. â€“ Local AI Smart Assistant

A.I.S.A. (Artificial Intelligent Smart Assistant) is a lightweight, local AI assistant designed to run on **low-spec computers** using an optimized open-source language model.

This project is focused on building a **fully offline, privacy-friendly AI assistant** capable of conversation, contextual memory, and local media control â€” with more features planned in future updates.

---

## ğŸš€ Features (Current)

* ğŸ’¬ Local conversational AI (powered by Mistral 7B)
* ğŸ§  Semantic memory with vector embeddings
* ğŸ“œ Persistent conversation history (`history.jsonl`)
* ğŸµ Local music control system:

  * Play playlist
  * Play specific song
  * Pause music
  * Resume music
  * Next track
* ğŸ” Context retrieval using cosine similarity
* ğŸ–¥ï¸ Designed for low-spec machines

---

## ğŸ§  Model Information

This project uses:

* **Mistral 7B (Quantized GGUF)**

  * Optimized version for low VRAM usage
  * Runs locally using `llama-cpp-python`
* **Sentence Transformers (all-MiniLM-L6-v2)**

  * Used for generating embeddings
  * Enables semantic search of past conversations

Everything runs **100% locally** â€” no API calls, no cloud dependency.

---

## ğŸ› ï¸ Tech Stack

* Python 3.10+
* llama-cpp-python
* sentence-transformers
* scikit-learn
* numpy
* threading
* Local music module (`music_player`)

---

## ğŸ“‚ Project Structure

```
AISA/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ history.jsonl
â”œâ”€â”€ music.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b-v0.1.Q4_K_M.gguf
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/AISA.git
cd AISA
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install llama-cpp-python
pip install sentence-transformers
pip install scikit-learn
pip install numpy
```

### 3ï¸âƒ£ Download the Model

Download a quantized GGUF version of:

```
mistral-7b-v0.1.Q4_K_M.gguf
```

Place it inside your project folder and update:

```python
model_path="your/model/path/mistral-7b-v0.1.Q4_K_M.gguf"
```

---

## â–¶ï¸ Running the Assistant

```bash
python main.py
```

Example commands:

```
play playlist chill
play song believer
pause music
resume music
next track
```

---

## ğŸ§  How Memory Works

* User inputs are converted into embeddings.
* Stored in `history.jsonl`.
* Cosine similarity retrieves the most relevant past messages.
* Relevant context is injected into the model prompt.

This enables:

* Context-aware responses
* Lightweight long-term memory
* Efficient local semantic recall

---

## ğŸ’» Designed for Low-Spec PCs

The assistant is optimized to:

* Run on quantized 7B models
* Use reduced VRAM via `n_gpu_layers`
* Operate within limited system RAM
* Avoid cloud processing

Tested on:

* 8â€“16GB RAM systems
* Mid-range GPUs
* CPU-only setups (with lower performance)

---

## ğŸ”® Planned Features

This is an ongoing project. Upcoming features may include:

* ğŸ”Š Voice input/output
* ğŸ—‚ï¸ File system interaction
* ğŸŒ Optional offline web search
* ğŸ–¥ï¸ GUI interface
* ğŸ” Encrypted memory storage
* ğŸ“Š Smarter memory indexing
* ğŸ§© Plugin system
* ğŸ  Smart home control
* ğŸ—“ï¸ Task and reminder system

---

## ğŸ¯ Project Goal

To create a **fully private, offline AI assistant** that anyone can run on affordable hardware â€” without relying on big tech APIs or cloud services.

---

## âš ï¸ Disclaimer

This project is experimental and under active development.
Performance depends on hardware and model quantization.

---

## ğŸ“œ License

This project uses open-source models and libraries.
Ensure you follow the license terms of:

* Mistral AI
* llama.cpp
* Sentence Transformers

---

## ğŸ¤ Contributing

Contributions are welcome!

If you'd like to:

* Improve performance
* Add features
* Optimize memory
* Improve modularity

Feel free to fork and submit a pull request.

---

## â­ Support the Project

If you like the idea of a fully local AI assistant for low-end hardware:

Give the repo a â­ and follow development!

---
